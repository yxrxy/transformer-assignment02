{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226f0dce-c88f-4aaf-8053-b7b104f0670a",
   "metadata": {},
   "source": [
    "# Q1（WordPiece 算法介绍）：\n",
    "    WordPiece是一种用于文本分词的算法，最初由Google在BERT模型中提出。其主要目标是解决词汇表外词汇（OOV）的问题，同时提高模型的表现。WordPiece通过将单词分解为更小的子词单位，使得即使是未见过的单词也能通过已有的子词组合来表示。\n",
    "### 具体实现步骤：\n",
    "初始化词汇表：\n",
    "从字符级别开始，词汇表最初只包含所有的单个字符和一些特殊符号（如[UNK]、[CLS]、[SEP]等）。\n",
    "\n",
    "统计词频：\n",
    "扫描训练数据，记录每个词的出现频率。\n",
    "\n",
    "生成子词单元：\n",
    "在每一轮迭代中，计算所有可能的子词对及其出现频率。\n",
    "选择频率最高的子词对，将其合并为一个新的子词，并更新词汇表。\n",
    "重复此过程，直到达到预设的词汇表大小或没有更多可合并的子词对。\n",
    "\n",
    "分词过程：\n",
    "对输入文本进行分词时，从左到右尝试匹配最长的子词单位，并逐步构建出完整的词汇表示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a64304-c6ce-4438-af24-fd954e0b6764",
   "metadata": {},
   "source": [
    "# Q2（使用SentencePiece在西游记数据集上训练分词器，西游记.txt_数据集-阿里云天池 (aliyun.com)，并进行测试）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443339f5-c783-4ebf-9e1b-87eed9932bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import jieba\n",
    "\n",
    "def train_sentencepiece(input_file, model_prefix, vocab_size):\n",
    "    spm.SentencePieceTrainer.train(f'--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size}')\n",
    "\n",
    "def load_sentencepiece_model(model_file):\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_file)\n",
    "    return sp\n",
    "\n",
    "def sentencepiece_tokenize(sp, text):\n",
    "    pieces = sp.encode(text, out_type=str)\n",
    "    return pieces\n",
    "\n",
    "def sentencepiece_detokenize(sp, pieces):\n",
    "    return sp.decode(pieces)\n",
    "\n",
    "def jieba_tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = '西游记.txt'\n",
    "    model_prefix = 'xiyouji'\n",
    "    vocab_size = 10000\n",
    "\n",
    "    train_sentencepiece(input_file, model_prefix, vocab_size)\n",
    "\n",
    "    sp = load_sentencepiece_model(f'{model_prefix}.model')\n",
    "\n",
    "    text = '''石猴笑道：“这股水乃是桥下冲贯石窍，倒挂下\n",
    "    来遮闭门户的。桥边有花有树，乃是一座石房。房内有石锅石\n",
    "    灶、石碗石盆、石床石凳，中间一块石碣上，镌着‘花果山福\n",
    "    地，水帘洞洞天’。真个是我们安身之处。里面且是宽阔，容\n",
    "    得千百口老小。我们都进去住，也省得受老天之气。'''\n",
    "\n",
    "    sp_pieces = sentencepiece_tokenize(sp, text)\n",
    "    print(\"SentencePiece 分词结果:\", sp_pieces)\n",
    "\n",
    "    # 反分词\n",
    "    sp_original_text = sentencepiece_detokenize(sp, sp_pieces)\n",
    "    print(\"SentencePiece 重构文本:\", sp_original_text)\n",
    "\n",
    "    jieba_segments = jieba_tokenize(text)\n",
    "    print(\"jieba 分词结果:\", jieba_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e38e1-142f-4593-bb72-cf2656a38ab3",
   "metadata": {},
   "source": [
    "## 运行结果："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da383b-cb39-4a87-807b-3c4bda24c854",
   "metadata": {},
   "source": [
    "![Alt](01py.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e33df5-8ce7-4e21-95da-e944c3419641",
   "metadata": {},
   "source": [
    "# Q3：请解释embedding层的作用，重点介绍为什么使用positional embedding以及positional embedding的原理\n",
    "在 Transformer 模型中，Embedding 层的作用是将离散的词汇（例如单词或子词）转换为连续的向量表示，以便神经网络能够处理这些数据。\n",
    "\n",
    "**词汇嵌入 (Word Embedding)**\n",
    "作用: 词汇嵌入的主要目的是将离散的词汇映射到一个高维向量空间中，每个词汇都有一个固定维度的向量表示。例如，如果词汇嵌入维度是 512，那么每个词汇将被表示为一个 512 维的向量。\n",
    " 通过这种方式，语义相似的词汇在向量空间中会接近，从而捕捉词汇之间的语义关系。它还可以减少词汇稀疏性，提高计算效率。\n",
    "\n",
    "**位置编码 (Positional Embedding)**\n",
    "自注意力机制的特点：\n",
    "Transformer 使用的是自注意力机制，它允许网络并行处理输入序列中的所有元素，而不是逐个处理。这带来了效率的提升，但同时也带来了一个问题：模型无法捕捉序列中元素的顺序信息。在自然语言处理中，词汇的顺序对于理解句子意义至关重要。例如，“我爱你”和“你爱我”虽然包含相同的词汇，但顺序不同，含义截然不同。如果模型无法捕捉到顺序信息，就很难正确理解和生成语言。位置编码通过正弦和余弦函数生成，确保对齐的两个位置编码之间具有一定的平滑变化和唯一性。这些编码的设计具有周期性和递增性，允许模型捕捉相对位置。正弦和余弦的组合确保了不同位置的编码是唯一的，并且编码之间的距离随位置增大而指数级变大，这有利于模型感知远距离依赖关系。在实际使用中，位置编码被添加到词汇嵌入上，得到一个既包含语义信息又包含位置信息的向量。这使得每个词汇的表示不仅反映了词汇本身的语义，也包含了它在序列中的位置。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56449d34-f37c-4c6d-acd5-5fbdb1e3c2e5",
   "metadata": {},
   "source": [
    "# Q4（选做）: 完成下列positional embedding接口的实现..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcffda2-fff4-4183-9928-a5285d2a24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, input_dim, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, input_dim, 2).float() * (-math.log(10000.0) / input_dim))\n",
    "        \n",
    "        pe = torch.zeros(max_len, input_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca340e-7971-4e4b-83eb-294d1819f67f",
   "metadata": {},
   "source": [
    "# Q5：如何理解注意力机制中的key、value、query的作用，试着从矩阵变化的角度来解释其输入X维度的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa7b48b-6ba6-49bf-b1a6-2ec766d7953e",
   "metadata": {},
   "source": [
    "我的理解是，Query是自己被修饰的各种可能的集合，Key是自己修饰他人的各种可能的集合。 比如creature的Q，和blue的K，两者做点积如果值较大就知道哦blue很可能是creature的修饰词。在注意力机制中，`query`、`key` 和 `value` 是关键概念，理解它们的作用有助于把握模型的工作原理。以下是从矩阵变化的角度对输入 `X` 维度变化的解释：\n",
    "\n",
    "### 1. 输入矩阵 `X`\n",
    "\n",
    "假设输入矩阵 `X` 的维度为 `(N, T, D)`，其中：\n",
    "- `N` 是批量大小（batch size）。\n",
    "- `T` 是序列长度（例如，词汇数）。\n",
    "- `D` 是每个输入向量的维度（特征维度）。\n",
    "\n",
    "### 2. 线性变换\n",
    "\n",
    "我们通过三个不同的权重矩阵将输入 `X` 投影到 `Q`、`K` 和 `V`：\n",
    "\n",
    "- **Query (Q)**: \n",
    "  \\[\n",
    "  $Q = XW_Q \\quad (N, T, D) \\times (D, d_k) \\rightarrow (N, T, d_k)$\n",
    "  \\]\n",
    "\n",
    "- **Key (K)**: \n",
    "  \\[\n",
    "  $K = XW_K \\quad (N, T, D) \\times (D, d_k) \\rightarrow (N, T, d_k)$\n",
    "  \\]\n",
    "\n",
    "- **Value (V)**: \n",
    "  \\[\n",
    "  $V = XW_V \\quad (N, T, D) \\times (D, d_v) \\rightarrow (N, T, d_v)$\n",
    "  \\]\n",
    "\n",
    "这里，`d_k` 和 `d_v` 是 `query` 和 `value` 的维度，通常是 `D` 的缩小维度。\n",
    "\n",
    "### 3. 注意力计算\n",
    "\n",
    "接下来，计算注意力得分：\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\]\n",
    "\n",
    "- **矩阵变化**：\n",
    "  - \\( K^T \\) 的维度变为 `(N, d_k, T)`，通过点积计算得分后，得到的结果为 `(N, T, T)`。\n",
    "  - 通过 softmax 处理后，得分用于加权 `V`，最终输出的维度为 `(N, T, d_v)`。\n",
    "\n",
    "### 4. 结果\n",
    "\n",
    "最终，`query` 通过与 `key` 的交互，决定了如何加权 `value`，以获得对每个位置的上下文信息。这一过程使得模型能够动态地聚焦于输入序列中不同的部分，从而增强表示能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778852b-3a32-4440-9e99-98417ad3e4a8",
   "metadata": {},
   "source": [
    " # Q6：完成简单版本的多头注意力的实现，接口如下：...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f575a-557e-4545-8f16-4517aed2c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(SimpleMultiHeadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        N, T, C = query.shape\n",
    "\n",
    "        Q = self.q_proj(query).view(N, T, self.num_heads, self.head_dim).transpose(1, 2)  # (N, num_heads, T, head_dim)\n",
    "        K = self.k_proj(key).view(N, T, self.num_heads, self.head_dim).transpose(1, 2)    # (N, num_heads, T, head_dim)\n",
    "        V = self.v_proj(value).view(N, T, self.num_heads, self.head_dim).transpose(1, 2)  # (N, num_heads, T, head_dim)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (N, num_heads, T, T)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # (N, num_heads, T, T)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)  # (N, num_heads, T, head_dim)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(N, T, self.embed_dim)  # (N, T, embed_dim)\n",
    "\n",
    "        output = self.out_proj(output)  # (N, T, embed_dim)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137cd20-240e-44aa-95e2-44d47db6491e",
   "metadata": {},
   "source": [
    "# Q7：请解释这样做的好处(在llama系列中对注意力机制做了相应的变化...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5932b-ec58-47e9-90f3-f70838b77d5e",
   "metadata": {},
   "source": [
    "1. 提高并行计算效率\n",
    "通过将 query 头分组，每组 query 头可以与对应的 key 和 value 并行计算，这样可以提高计算效率和速度。同时，这也可以更好地利用硬件资源，如 GPU 和 TPU 的并行计算能力。\n",
    "\n",
    "2. 减少计算开销\n",
    "因为每组 query 头仅与对应的 key 和 value 交互，而不是所有头都共享一组 key 和 value，计算量会相应减少，特别是在处理长序列输入时，效率提升明显。\n",
    "\n",
    "3. 细粒度的上下文捕获\n",
    "细化到组内部的注意力机制提供了一种细粒度的方式来捕获不同上下文之间的关系。通过分组，不同的 query 头组可以专注于序列中不同的方面或特征，从而细致地捕获并表示上下文信息。\n",
    "\n",
    "4. 增强模型的多样性\n",
    "分组的 query 头与不同的 key 和 value 组合可以增加模型的多样性。不同组的 query 头可以捕获到不同层次和不同类型的关系信息，从而提升模型的通用性和表现力。\n",
    "\n",
    "5. 更灵活的注意力机制\n",
    "通过将 query 头分组，可以更灵活地调整每组注意力机制的配置。例如，不同的组可以选择不同层次的语义或不同类型的特征，从而增强整体模型的灵活性和适应性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa90f09-aa24-4b4f-a5ec-33cfb8980330",
   "metadata": {},
   "source": [
    "# Q8：完成GQA的具体实现，接口与SimpleMultiHeadAttention 一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6656e2-47e4-4520-b796-1297731f2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_groups):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \n",
    "        assert num_heads % num_groups == 0, \n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        N, T, _ = query.shape\n",
    "        group_size = self.num_heads // self.num_groups\n",
    "\n",
    "        Q = self.q_proj(query).view(N, T, self.num_heads, self.head_dim).transpose(1, 2)  # (N, num_heads, T, head_dim)\n",
    "        K = self.k_proj(key).view(N, T, self.num_heads, self.head_dim).transpose(1, 2)    # (N, num_heads, T, head_dim)\n",
    "        V = self.v_proj(value).view(N, T, self.num_heads, self.head_dim).transpose(1, 2)  # (N, num_heads, T, head_dim)\n",
    "\n",
    "        outputs = []\n",
    "        attn_weights = []\n",
    "\n",
    "        for g in range(self.num_groups):\n",
    "            q_group = Q[:, g * group_size:(g + 1) * group_size, :, :]  # (N, group_size, T, head_dim)\n",
    "            k_group = K[:, g * group_size:(g + 1) * group_size, :, :]  # (N, group_size, T, head_dim)\n",
    "            v_group = V[:, g * group_size:(g + 1) * group_size, :, :]  # (N, group_size, T, head_dim)\n",
    "\n",
    "            scores = torch.matmul(q_group, k_group.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (N, group_size, T, T)\n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            attn_weights_group = F.softmax(scores, dim=-1)  # (N, group_size, T, T)\n",
    "            attn_weights.append(attn_weights_group)\n",
    "\n",
    "            output_group = torch.matmul(attn_weights_group, v_group)  # (N, group_size, T, head_dim)\n",
    "            outputs.append(output_group)\n",
    "\n",
    "        output = torch.cat(outputs, dim=1)  # (N, num_heads, T, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(N, T, self.embed_dim)  # (N, T, embed_dim)\n",
    "        output = self.out_proj(output)  # (N, T, embed_dim)\n",
    "\n",
    "        attn_weights = torch.cat(attn_weights, dim=1)  # (N, num_heads, T, T)\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44706af4-ad97-43b2-bb1a-b9f105f0fbad",
   "metadata": {},
   "source": [
    "# Q9：完成Feed-Forward层的代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedde615-ef3c-4e5d-a807-51c9b3b944d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb9615-2d7d-490a-8779-286a2d46261a",
   "metadata": {},
   "source": [
    "# Q10： 介绍Layer Normalization的原理，并分析它与batch normalization的区别，以及为什么Transformer 用Layer Normalization而不用batch normalization？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997082f3-1494-4179-b58d-1358d9b7167f",
   "metadata": {},
   "source": [
    "Layer Normalization（层归一化）原理\n",
    "1. 定义\n",
    "Layer Normalization（层归一化）在神经网络中作用于每个数据点，而不是像 Batch Normalization（批次归一化）那样作用于整个 mini-batch。具体来说，Layer Normalization 在计算归一化时，对于每个输入样本的特征维度进行归一化。\n",
    "\n",
    "2. 数学公式\n",
    "给定输入 x，形状为 (batch_size, seq_len, feature_dim)，对于每个特征维度 i，Layer Normalization 的计算步骤如下：\n",
    "\n",
    "计算均值：\n",
    "$   \\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i\n",
    "  $\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "计算方差：\n",
    "$   \\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2\n",
    "  $\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "归一化：\n",
    "$   \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "  $\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "缩放和平移：\n",
    "$   y_i = \\gamma \\hat{x}_i + \\beta\n",
    "  $\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "其中：\n",
    "\n",
    "H 是特征维度的大小。\n",
    "$\\epsilon$ 是一个小常数，防止分母为零。\n",
    "$\\gamma$ 和 $\\beta$ 是可学习的参数，用于缩放和平移。\n",
    "与 Batch Normalization 的区别\n",
    "1. 计算范围\n",
    "Batch Normalization：在 mini-batch 上计算均值和方差，每个特征在整个 mini-batch 上进行归一化。\n",
    "$  \\hat{x}^{(k)} = \\frac{x^{(k)} - \\mu_B^{(k)}}{\\sqrt{(\\sigma_B^{(k)})^2 + \\epsilon}}\n",
    " $\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "其中，k 表示特征维度，B 表示 mini-batch 的大小。\n",
    "\n",
    "Layer Normalization：在每个单独的样本的特征维度上计算均值和方差，即在特征维度 H 上进行归一化。\n",
    "$  \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    " $\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "其中，i 表示特征维度。\n",
    "\n",
    "2. 适用场景\n",
    "Batch Normalization：由于在 mini-batch 上计算归一化参数，适用于卷积神经网络（CNN）和全连接网络（FCN），能够利用大批量数据的统计特性。\n",
    "Layer Normalization：独立于 mini-batch，适用于循环神经网络（RNN）和 Transformer 等，需要对每个时间步或输入样本进行归一化的网络。\n",
    "为什么 Transformer 使用 Layer Normalization 而不是 Batch Normalization\n",
    "序列建模的独立性：\n",
    "\n",
    "Transformer 模型中的每个输入序列在处理时独立于其他序列。Batch Normalization 依赖于 mini-batch 的统计信息，而序列间可能不具有一致性，这会引入噪声。\n",
    "Layer Normalization 在每个单独样本的特征上进行归一化，与其他样本无关，适合序列建模任务。\n",
    "在线预测和序列处理：\n",
    "\n",
    "在预测阶段，特别是在线预测时，可能只有一个样本输入，Batch Normalization 无法有效计算 mini-batch 的统计特性。\n",
    "Layer Normalization 不依赖于 mini-batch，从而在处理单个样本时更加稳定和有效。\n",
    "全局依赖特性的捕捉：\n",
    "\n",
    "Transformer 需要捕捉输入序列中全局依赖关系，而不是局部依赖。Layer Normalization 能够更好地保留序列全局信息，而 Batch Normalization 更适合捕捉局部统计特性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69806d93-6bdd-4134-b34e-5497141000e5",
   "metadata": {},
   "source": [
    "# Q11（选做）：实现Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c617866-bf4a-4e9c-a0a4-68f42393df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, dim)\n",
    "\n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, seq_len, dim)\n",
    "        \"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "\n",
    "        out = (x - mean) / (std + self.eps)\n",
    "\n",
    "        out = self.gamma * out + self.beta\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177a477-ee19-4cf0-91b7-e6c9264f1a02",
   "metadata": {},
   "source": [
    "# Q12：现在你应该可以完成整个transformer的搭建工作，给定相应接口完成transformer的构建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513a5d4-60aa-40ce-a20d-775105381fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    ff_hidden_dim: int\n",
    "    num_layers: int\n",
    "    input_dim: int\n",
    "    output_dim: int\n",
    "    dropout: float = 0.1\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        out = (x - mean) / (std + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layernorm1 = LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layernorm2 = LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(params.input_dim, params.embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(params.embed_dim, params.dropout)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerBlock(i, params.embed_dim, params.num_heads, params.ff_hidden_dim, params.dropout)\n",
    "            for i in range(params.num_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerBlock(i, params.embed_dim, params.num_heads, params.ff_hidden_dim, params.dropout)\n",
    "            for i in range(params.num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(params.embed_dim, params.output_dim)\n",
    "\n",
    "    def forward(self, input_tokens: torch.Tensor):\n",
    "        input_embedded = self.embedding(input_tokens) * torch.sqrt(torch.tensor(self.params.embed_dim))\n",
    "        input_encoded = self.positional_encoding(input_embedded)\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            input_encoded = encoder_layer(input_encoded, mask=None) \n",
    "        \n",
    "        output_encoded = input_encoded \n",
    "        \n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            output_encoded = decoder_layer(output_encoded, mask=None) \n",
    "        \n",
    "        output = self.fc_out(output_encoded)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    ff_hidden_dim=128,\n",
    "    num_layers=6,\n",
    "    input_dim=10,\n",
    "    output_dim=10,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transformer_block = TransformerBlock(\n",
    "        layer_id=0,\n",
    "        embed_dim=model_args.embed_dim,\n",
    "        num_heads=model_args.num_heads,\n",
    "        ff_hidden_dim=model_args.ff_hidden_dim,\n",
    "        dropout=model_args.dropout\n",
    "    )\n",
    "\n",
    "    input_tensor = torch.rand(10, 2, model_args.embed_dim)  # (seq_len, batch_size, embed_dim)\n",
    "    mask = None \n",
    "\n",
    "    output = transformer_block(input_tensor, mask)\n",
    "    print(\"Output:\", output) \n",
    "    print(\"Output shape:\", output.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71960d7d-a056-4b9e-b0e3-67aec973151c",
   "metadata": {},
   "source": [
    "### 运行结果(**Q13**)："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c299a-826c-42cb-b650-cfbfac1f3ad8",
   "metadata": {},
   "source": [
    "![ALT](02py.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7afb27-1b24-4d88-91fd-289cc87c7ae5",
   "metadata": {},
   "source": [
    "# Q13：尝试在你构建的transformer上训练数据，可以参考d2l中的做"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5aa29-0865-4cdb-b1a0-e496f26050a5",
   "metadata": {},
   "source": [
    "    Beam Search是一种用于序列生成的启发式搜索算法，它在生成每个单词或符号时，都保留了一组（即“束”或\"beam\"）最有潜力的部分序列，而不是只选择当前概率最高的一个。这种方法可以有效地提高输出的质量，尤其是在机器翻译、文本摘要等任务中。\n",
    "\n",
    "**Beam Search的基本步骤：**\n",
    "初始化：开始时，将初始序列（通常是开始符号）加入到beam中。\n",
    "迭代展开：对于beam中的每个序列，计算所有可能的下一个token的概率，并选择概率最高的num_beams个序列加入到新的beam中。\n",
    "完成序列：当序列达到预定的长度或生成了结束符号时，将其从beam中移出，并作为候选的完成序列。\n",
    "输出选择：在所有候选序列中选择最优的一个或多个序列作为输出。通常是基于序列的总体概率（如对数概率的和）来选择。\n",
    "参数解释：\n",
    "    \n",
    "1. num_beams：\n",
    "作用：定义了在每一步搜索中保留的序列数量。这个参数直接影响了搜索的空间大小。\n",
    "值：一个正整数，例如num_beams=5表示在每一步都保留概率最高的5个序列。\n",
    "2. top_k：\n",
    "作用：在为每个序列生成下一个token时，只考虑概率最高的top_k个token。这可以避免考虑太多低概率的token，从而加速搜索过程。\n",
    "值：一个正整数，例如top_k=50。\n",
    "3. top_p（也称为核采样）：\n",
    "作用：与top_k类似，但是它考虑的是概率质量而不是固定数量的token。具体来说，它只考虑累积概率达到top_p的token。\n",
    "值：一个0到1之间的小数，例如top_p=0.9。\n",
    "4. temperature：\n",
    "作用：用于调节概率分布的平滑程度。较高的temperature值会使概率分布更加平滑，从而增加随机性和多样性；较低的temperature值会使得模型更加自信，输出更确定的预测。\n",
    "值：一个大于0的数，通常接近1，例如temperature=1.0表示不改变分布，而temperature=0.5表示更倾向于高概率的token。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a3aa3-f872-4617-950f-844b33f98317",
   "metadata": {},
   "source": [
    "# Q15（选做）：下列代码节选自某框架，请根据上一题的学习完成代码补全...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518a366-131c-4426-a5b1-eb8e3c6de531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 假设 logits 是模型生成的概率分布，其已经通过softmax处理\n",
    "# logits.shape == (batch_size, seq_len, vocab_size)\n",
    "logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos=cur_pos)\n",
    "\n",
    "if temperature > 0:\n",
    "    logits = logits / temperature\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "else:\n",
    "    probs = logits\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    \"\"\"\n",
    "    Perform top-p (nucleus) sampling on a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        probs (torch.Tensor): Probability distribution tensor.\n",
    "        p (float): Probability threshold for top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled token indices.\n",
    "\n",
    "    Note:\n",
    "        Top-p sampling selects the smallest set of tokens whose cumulative \n",
    "        probability mass exceeds the threshold p. The distribution is renormalized \n",
    "        based on the selected tokens.\n",
    "    \"\"\"\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    if sorted_indices_to_remove.dim() > 1:\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    sorted_probs[sorted_indices_to_remove] = 0\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
    "    return sorted_indices.gather(-1, next_token)\n",
    "\n",
    "temperatures = 1.0\n",
    "top_p = 0.9\n",
    "\n",
    "probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "next_tokens = sample_top_p(probs, top_p)\n",
    "\n",
    "# Update tokens with next_tokens\n",
    "tokens = torch.cat([tokens, next_tokens], dim=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
